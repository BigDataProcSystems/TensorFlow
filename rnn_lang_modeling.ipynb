{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:18pt; padding-top:20px; text-align:center\"><b>Рекуррентная нейронная сеть и </b> <span style=\"font-weight:bold; color:green\">TensorFlow</span></div><hr>\n",
    "<div style=\"text-align:right;\">Папулин С.Ю. <span style=\"font-style: italic;font-weight: bold;\">(papulin_hse@mail.ru)</span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Подключение стилей оформления</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<link href=\"css/style.css\" rel=\"stylesheet\" type=\"text/css\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import inspect\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lib import rnn_lang_modeling_reader as reader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "<div style=\"display:table; width:100%; padding-top:10px; padding-bottom:10px; border-bottom:1px solid lightgrey\">\n",
    "    <div style=\"display:table-row\">\n",
    "        <div style=\"display:table-cell; width:80%; font-size:14pt; font-weight:bold\">1. Загрузка исходных данных</div>\n",
    "    \t<div style=\"display:table-cell; width:20%; text-align:center; background-color:whitesmoke; border:1px solid lightgrey\"><a href=\"#0\">К содержанию</a></div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>Вариант 1.</b> Из командной строки</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!wget http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!tar xvf simple-examples.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>Вариант 2.</b> Средствами Python</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import shutil\n",
    "import os\n",
    "import tarfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = \"http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\"\n",
    "\n",
    "filename = \"data/rnn-lang-modeling/rnn-simple.tgz\"\n",
    "\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "\n",
    "# Загрузка архива\n",
    "with urllib.request.urlopen(url) as response:\n",
    "    with open(filename, 'wb') as output:\n",
    "        shutil.copyfileobj(response, output)\n",
    "\n",
    "# Распаковка\n",
    "with tarfile.open(filename) as tar:\n",
    "    tar.extractall(path=\"data/rnn-lang-modeling/\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Директория с исходными данными и для записи логов и модели</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = \"data/rnn-lang-modeling/simple-examples/data\"\n",
    "save_path = \"log/rnn-lang-modeling/log\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><i>Загрузка данных и преобразование в вектор индексов слов</i></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data, vocabulary = reader.ptb_raw_data(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><i>Количество слов (токенов) в обучающем подмножестве</i></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><i>Индекс первого слова в тестовом подмножестве</i></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><i>Словарь преобразования слов в индексы</i></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><i>Обратное пребразование индекса первого слова тестового подмножества</i></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for key in vocabulary:\n",
    "    if vocabulary[key] == test_data[0]:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n",
    "<div style=\"display:table; width:100%; padding-top:10px; padding-bottom:10px; border-bottom:1px solid lightgrey\">\n",
    "    <div style=\"display:table-row\">\n",
    "        <div style=\"display:table-cell; width:80%; font-size:14pt; font-weight:bold\">2. Этапы построения сети</div>\n",
    "    \t<div style=\"display:table-cell; width:20%; text-align:center; background-color:whitesmoke; border:1px solid lightgrey\"><a href=\"#0\">К содержанию</a></div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Параметры модели</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><i>Структура сети</i></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_steps = 24 # количество развертки LSTM\n",
    "hidden_size = 200 # количество LSTM единиц\n",
    "vocab_size = 10000 # размер словаря"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><i>Параметры инициализации весов и параметры обучения</i></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init_scale = 0.1 # the initial scale of the weights\n",
    "learning_rate = 1.0 # коэффициент обучения\n",
    "max_grad_norm = 5 # the maximum permissible norm of the gradient\n",
    "max_epoch = 4 # the number of epochs trained with the initial learning rate\n",
    "max_max_epoch = 13 # количество эпох\n",
    "keep_prob = 1.0 # вероятность сохранения веса в dropout слое\n",
    "lr_decay = 0.5 # the decay of the learning rate for each epoch after \"max_epoch\"\n",
    "batch_size = 20 # размер batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Построение сети</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><i>Входные данные</i></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><i>Формирование исходных данных</i></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train, y_train = reader.ptb_producer(train_data, batch_size, num_steps, name=None)\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><i>Преобразование слов в распределенное представление (word2vec)</i></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding = tf.get_variable(\"embedding\", [vocab_size, hidden_size], dtype=tf.float32)\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputs = tf.nn.embedding_lookup(embedding, x_train)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><i>Создание двух слоев LSTM</i></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lstm_layer_1 = tf.contrib.rnn.BasicLSTMCell(hidden_size, forget_bias=0.0, \n",
    "                                            state_is_tuple=True,\n",
    "                                            reuse=tf.get_variable_scope().reuse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm_layer_2 = tf.contrib.rnn.BasicLSTMCell(hidden_size, forget_bias=0.0, \n",
    "                                            state_is_tuple=True, \n",
    "                                            reuse=tf.get_variable_scope().reuse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lstm_layer_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><i>Объединение слоев в одну структуру</i></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "multiple_cell = tf.contrib.rnn.MultiRNNCell([lstm_layer_1, lstm_layer_2], state_is_tuple=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><i>Инициализация начальных значений состояний</i></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "state = multiple_cell.zero_state(batch_size, tf.float32)\n",
    "state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><i>Определение начальных и выходных состояний</i></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputs = tf.unstack(inputs, num=num_steps, axis=1)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputs, state = tf.contrib.rnn.static_rnn(multiple_cell, inputs, initial_state=state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "state[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#outputs = []\n",
    "#with tf.variable_scope(\"RNN\"):\n",
    "#    for time_step in range(num_steps):\n",
    "        \n",
    "        #if time_step > 0: \n",
    "            # \n",
    "        #    tf.get_variable_scope().reuse_variables()\n",
    "            \n",
    "#        cell_output, state = multiple_cell(inputs[:, time_step, :], state)\n",
    "#        outputs.append(cell_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><i>Вычисление вероятности появления данных из y_train</i></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output = tf.reshape(tf.stack(axis=1, values=outputs), [-1, hidden_size])\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "24*20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "softmax_w = tf.get_variable(\"softmax_w\", [hidden_size, vocab_size], dtype=tf.float32)\n",
    "softmax_b = tf.get_variable(\"softmax_b\", [vocab_size], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logits = tf.matmul(output, softmax_w) + softmax_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example([logits],\n",
    "                                                          [tf.reshape(y_train, [-1])], \n",
    "                                                          [tf.ones([batch_size * num_steps], dtype=tf.float32)])\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><i>Функция потерь</i></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost = tf.reduce_sum(loss) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><i>Конечное состояние</i></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_state = state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3\"></a>\n",
    "<div style=\"display:table; width:100%; padding-top:10px; padding-bottom:10px; border-bottom:1px solid lightgrey\">\n",
    "    <div style=\"display:table-row\">\n",
    "        <div style=\"display:table-cell; width:80%; font-size:14pt; font-weight:bold\">3. Запуск обучения</div>\n",
    "    \t<div style=\"display:table-cell; width:20%; text-align:center; background-color:whitesmoke; border:1px solid lightgrey\"><a href=\"#0\">К содержанию</a></div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Параметры сети</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SmallConfig(object):\n",
    "    \"\"\"Small config.\"\"\"\n",
    "    init_scale = 0.1 # the initial scale of the weights\n",
    "    learning_rate = 1.0 # the initial value of the learning rate\n",
    "    max_grad_norm = 5 # the maximum permissible norm of the gradient\n",
    "    num_layers = 2 # the number of LSTM layers\n",
    "    num_steps = 20 # the number of unrolled steps of LSTM\n",
    "    hidden_size = 200 # the number of LSTM units\n",
    "    max_epoch = 4 # the number of epochs trained with the initial learning rate\n",
    "    max_max_epoch = 13 # the total number of epochs for training\n",
    "    keep_prob = 1.0 # the probability of keeping weights in the dropout layer\n",
    "    lr_decay = 0.5 # the decay of the learning rate for each epoch after \"max_epoch\"\n",
    "    batch_size = 20 # the batch size\n",
    "    vocab_size = 10000 # the vocabulary size\n",
    "\n",
    "class TestConfig(object):\n",
    "    \"\"\"Tiny config, for testing.\"\"\"\n",
    "    init_scale = 0.1\n",
    "    learning_rate = 1.0\n",
    "    max_grad_norm = 1\n",
    "    num_layers = 1\n",
    "    num_steps = 2\n",
    "    hidden_size = 2\n",
    "    max_epoch = 1\n",
    "    max_max_epoch = 1\n",
    "    keep_prob = 1.0\n",
    "    lr_decay = 0.5\n",
    "    batch_size = 20\n",
    "    vocab_size = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Входные данные</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PTBInput(object):\n",
    "    \"\"\"The input data.\"\"\"\n",
    "\n",
    "    def __init__(self, config, data, name=None):\n",
    "        self.batch_size = batch_size = config.batch_size\n",
    "        self.num_steps = num_steps = config.num_steps\n",
    "        self.epoch_size = ((len(data) // batch_size) - 1) // num_steps\n",
    "        self.input_data, self.targets = reader.ptb_producer(data, batch_size, num_steps, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Модель сети</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class PTBModel(object):\n",
    "    \n",
    "    \"\"\"The PTB model.\"\"\"\n",
    "\n",
    "    def __init__(self, is_training, config, input_):\n",
    "        \n",
    "        self._input = input_\n",
    "        \n",
    "        batch_size = input_.batch_size\n",
    "        num_steps = input_.num_steps\n",
    "        size = config.hidden_size\n",
    "        vocab_size = config.vocab_size\n",
    "\n",
    "        # Slightly better results can be obtained with forget gate biases\n",
    "        # initialized to 1 but the hyperparameters of the model would need to be\n",
    "        # different than reported in the paper.\n",
    "        \n",
    "        def lstm_cell():\n",
    "            return tf.contrib.rnn.BasicLSTMCell(size, \n",
    "                                                forget_bias=0.0, \n",
    "                                                state_is_tuple=True,\n",
    "                                                reuse=tf.get_variable_scope().reuse)\n",
    "            \n",
    "        attn_cell = lstm_cell\n",
    "        \n",
    "        \n",
    "        if is_training and config.keep_prob < 1:\n",
    "            def attn_cell():\n",
    "                return tf.contrib.rnn.DropoutWrapper(lstm_cell(), output_keep_prob=config.keep_prob)\n",
    "        \n",
    "        cell = tf.contrib.rnn.MultiRNNCell([attn_cell() for _ in range(config.num_layers)], \n",
    "                                           state_is_tuple=True)\n",
    "\n",
    "        self._initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "        \n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            embedding = tf.get_variable(\"embedding\", [vocab_size, size], dtype=tf.float32)\n",
    "            inputs = tf.nn.embedding_lookup(embedding, input_.input_data)\n",
    "\n",
    "        if is_training and config.keep_prob < 1:\n",
    "            inputs = tf.nn.dropout(inputs, config.keep_prob)\n",
    "\n",
    "        # Simplified version of models/tutorials/rnn/rnn.py's rnn().\n",
    "        # This builds an unrolled LSTM for tutorial purposes only.\n",
    "        # In general, use the rnn() or state_saving_rnn() from rnn.py.\n",
    "        #\n",
    "        # The alternative version of the code below is:\n",
    "        #\n",
    "        # inputs = tf.unstack(inputs, num=num_steps, axis=1)\n",
    "        # outputs, state = tf.contrib.rnn.static_rnn(\n",
    "        #     cell, inputs, initial_state=self._initial_state)\n",
    "        \n",
    "        outputs = []\n",
    "        state = self._initial_state\n",
    "        with tf.variable_scope(\"RNN\"):\n",
    "            for time_step in range(num_steps):\n",
    "                if time_step > 0: \n",
    "                    tf.get_variable_scope().reuse_variables()\n",
    "                (cell_output, state) = cell(inputs[:, time_step, :], state)\n",
    "                outputs.append(cell_output)\n",
    "\n",
    "        output = tf.reshape(tf.stack(axis=1, values=outputs), [-1, size])\n",
    "        softmax_w = tf.get_variable(\"softmax_w\", \n",
    "                                    [size, vocab_size], \n",
    "                                    dtype=tf.float32)\n",
    "        softmax_b = tf.get_variable(\"softmax_b\", \n",
    "                                    [vocab_size], \n",
    "                                    dtype=tf.float32)\n",
    "        logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "        loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example([logits],\n",
    "                                                                  [tf.reshape(input_.targets, [-1])],\n",
    "                                                                  [tf.ones([batch_size * num_steps], \n",
    "                                                                           dtype=tf.float32)])\n",
    "        self._cost = cost = tf.reduce_sum(loss) / batch_size\n",
    "        self._final_state = state\n",
    "\n",
    "        if not is_training:\n",
    "            return\n",
    "\n",
    "        self._lr = tf.Variable(0.0, trainable=False)\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars),\n",
    "                                          config.max_grad_norm)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(self._lr)\n",
    "        self._train_op = optimizer.apply_gradients(zip(grads, tvars),\n",
    "                                                   global_step=tf.contrib.framework.get_or_create_global_step())\n",
    "\n",
    "        self._new_lr = tf.placeholder(tf.float32, shape=[], name=\"new_learning_rate\")\n",
    "        self._lr_update = tf.assign(self._lr, self._new_lr)\n",
    "\n",
    "    def assign_lr(self, session, lr_value):\n",
    "        session.run(self._lr_update, feed_dict={self._new_lr: lr_value})\n",
    "\n",
    "    @property\n",
    "    def input(self):\n",
    "        return self._input\n",
    "\n",
    "    @property\n",
    "    def initial_state(self):\n",
    "        return self._initial_state\n",
    "\n",
    "    @property\n",
    "    def cost(self):\n",
    "        return self._cost\n",
    "\n",
    "    @property\n",
    "    def final_state(self):\n",
    "        return self._final_state\n",
    "\n",
    "    @property\n",
    "    def lr(self):\n",
    "        return self._lr\n",
    "\n",
    "    @property\n",
    "    def train_op(self):\n",
    "        return self._train_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Запуск эпохи</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_epoch(session, model, eval_op=None, verbose=False):\n",
    "    \"\"\"Runs the model on the given data.\"\"\"\n",
    "    start_time = time.time()\n",
    "    costs = 0.0\n",
    "    iters = 0\n",
    "    state = session.run(model.initial_state)\n",
    "\n",
    "    fetches = {\n",
    "      \"cost\": model.cost,\n",
    "      \"final_state\": model.final_state,\n",
    "    }\n",
    "    if eval_op is not None:\n",
    "        fetches[\"eval_op\"] = eval_op\n",
    "\n",
    "    for step in range(model.input.epoch_size):\n",
    "        feed_dict = {}\n",
    "        \n",
    "        #cell state and hidden state\n",
    "        for i, (c, h) in enumerate(model.initial_state):\n",
    "            feed_dict[c] = state[i].c\n",
    "            feed_dict[h] = state[i].h\n",
    "\n",
    "            \n",
    "        # i = input_gate, j = new_input, f = forget_gate, o = output_gate\n",
    "        # concat = _conv_linear([inputs, h], self.filter_size, self.num_features * 4, True)\n",
    "        # i, j, f, o = tf.split(axis=3, num_or_size_splits=4, value=concat)\n",
    "            \n",
    "        \n",
    "        vals = session.run(fetches, feed_dict)\n",
    "        cost = vals[\"cost\"]\n",
    "        state = vals[\"final_state\"]\n",
    "\n",
    "        costs += cost\n",
    "        iters += model.input.num_steps\n",
    "\n",
    "        if verbose and step % (model.input.epoch_size // 10) == 10:\n",
    "            print(\"%.3f perplexity: %.3f speed: %.0f wps\" %\n",
    "                (step * 1.0 / model.input.epoch_size, np.exp(costs / iters),\n",
    "                 iters * model.input.batch_size / (time.time() - start_time)))\n",
    "\n",
    "    return np.exp(costs / iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Конфигурирование</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "config = SmallConfig()\n",
    "config.max_max_epoch=13\n",
    "\n",
    "eval_config = SmallConfig()\n",
    "eval_config.batch_size = 1\n",
    "eval_config.num_steps = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Запуск обучения</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Graph().as_default():\n",
    "    \n",
    "    initializer = tf.random_uniform_initializer(-config.init_scale, config.init_scale)\n",
    "    \n",
    "    with tf.name_scope(\"Train\"):\n",
    "        \n",
    "        train_input = PTBInput(config=config, data=train_data, name=\"TrainInput\")\n",
    "        \n",
    "        with tf.variable_scope(\"Model\", reuse=None, initializer=initializer):\n",
    "            m = PTBModel(is_training=True, config=config, input_=train_input)\n",
    "        \n",
    "        tf.summary.scalar(\"Training_Loss\", m.cost)\n",
    "        tf.summary.scalar(\"Learning_Rate\", m.lr)\n",
    "        \n",
    "    with tf.name_scope(\"Valid\"):\n",
    "        \n",
    "        valid_input = PTBInput(config=config, data=valid_data, name=\"ValidInput\")\n",
    "        \n",
    "        with tf.variable_scope(\"Model\", reuse=True, initializer=initializer):\n",
    "            mvalid = PTBModel(is_training=False, config=config, input_=valid_input)\n",
    "        \n",
    "        tf.summary.scalar(\"Validation_Loss\", mvalid.cost)\n",
    "        \n",
    "    with tf.name_scope(\"Test\"):\n",
    "        \n",
    "        test_input = PTBInput(config=eval_config, data=test_data, name=\"TestInput\")\n",
    "        \n",
    "        with tf.variable_scope(\"Model\", reuse=True, initializer=initializer):\n",
    "            mtest = PTBModel(is_training=False, config=eval_config, input_=test_input)\n",
    "\n",
    "    sv = tf.train.Supervisor(logdir=save_path)\n",
    "    \n",
    "    with sv.managed_session() as session:\n",
    "        for i in range(config.max_max_epoch):\n",
    "            lr_decay = config.lr_decay ** max(i + 1 - config.max_epoch, 0.0)\n",
    "            m.assign_lr(session, config.learning_rate * lr_decay)\n",
    "            print(\"Epoch: %d Learning rate: %.3f\" % (i + 1, session.run(m.lr)))\n",
    "            train_perplexity = run_epoch(session, m, eval_op=m.train_op,\n",
    "                                     verbose=True)\n",
    "            print(\"Epoch: %d Train Perplexity: %.3f\" % (i + 1, train_perplexity))\n",
    "            valid_perplexity = run_epoch(session, mvalid)\n",
    "            print(\"Epoch: %d Valid Perplexity: %.3f\" % (i + 1, valid_perplexity))\n",
    "        \n",
    "        test_perplexity = run_epoch(session, mtest)\n",
    "        print(\"Test Perplexity: %.3f\" % test_perplexity)\n",
    "        \n",
    "        \n",
    "        if save_path:\n",
    "            print(\"Saving model to %s.\" % save_path)\n",
    "            sv.saver.save(session, save_path, global_step=sv.global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<p>Восстановление модели</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ".meta - граф\n",
    ".data - значения весов, переменных\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"4\"></a>\n",
    "<div style=\"display:table; width:100%; padding-top:10px; padding-bottom:10px; border-bottom:1px solid lightgrey\">\n",
    "    <div style=\"display:table-row\">\n",
    "        <div style=\"display:table-cell; width:80%; font-size:14pt; font-weight:bold\">4. Источники</div>\n",
    "    \t<div style=\"display:table-cell; width:20%; text-align:center; background-color:whitesmoke; border:1px solid lightgrey\"><a href=\"#0\">К содержанию</a></div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "https://github.com/tensorflow/models/tree/master/tutorials/rnn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
