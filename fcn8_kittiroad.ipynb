{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Семантическая сегментация изображений на основе полностью сверточной нейронной сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"0\"></a>\n",
    "<div><span style=\"font-size:14pt; font-weight:bold\">Содержание</span>\n",
    "    <ol>\n",
    "        <li><a href=\"#1\">Параметры</a></li>\n",
    "        <li><a href=\"#2\">Загрузка датасета KittyRoad</a></li>\n",
    "        <li><a href=\"#3\">Загрузка сверточной нейронной сети классификации VGG16</a></li>\n",
    "        <li><a href=\"#4\">Работа с данными</a></li>\n",
    "        <li><a href=\"#5\">Полностью сверточная нейронная сеть FCN8</a>\n",
    "            <ol style = \"list-style-type:lower-alpha\">\n",
    "                <li><a href=\"#5a\">Кодер</a></li>\n",
    "                <li><a href=\"#5b\">Декодер</a></li>\n",
    "                <li><a href=\"#5c\">Обучение и тестирование</a></li>\n",
    "            </ol>\n",
    "        </li>\n",
    "        <li><a href=\"#6\">Источники</a>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from urllib.request import urlretrieve\n",
    "import shutil\n",
    "import zipfile\n",
    "from glob import glob\n",
    "from glob import glob1\n",
    "import re\n",
    "import numpy as np\n",
    "import scipy.misc\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### !!! Сеть будет обучаться достаточно долго. Данную тетрадь можно запустить в облачном сервисе Google Colaboratory https://colab.research.google.com/ и значительно сократить время обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "## 1. Параметры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Путь к предобученной модели VGG16\n",
    "vgg16_path = \"./lib/vgg16\"\n",
    "# Путь к датасету\n",
    "dataset_path = \"./data/fcn8-kittiroad/data_road\"\n",
    "# Путь для сохранения сегментированных картинок\n",
    "out_path = \"./out/fcn8-kittiroad\"\n",
    "# Путь для сохранения обученной нейронной сети\n",
    "model_path=\"./models/fcn8-kittiroad/fcn8\"\n",
    "# Количество классов сегментации (дорога и не дорога)\n",
    "num_classes = 2\n",
    "# Размер входного изображения\n",
    "image_shape = (160, 576)\n",
    "# Размер батча\n",
    "batch_size = 16\n",
    "# Количество эпох обучения\n",
    "epochs = 15\n",
    "# Скорость(шаг) обучения\n",
    "lr = 0.0001\n",
    "# Уровень dropout-регуляризации\n",
    "dropout = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n",
    "## 2. Загрузка датасета KittiRoad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget -P data/fcn8-kittiroad/ https://s3.eu-central-1.amazonaws.com/avg-kitti/data_road.zip\n",
    "! unzip data/fcn8-kittiroad/data_road.zip -d data/fcn8-kittiroad/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3\"></a>\n",
    "## 3. Загрузка сверточной нейронной сети классификации VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Код для загрузки предобученной сверточной нейронной сети классификации VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dMg22NflK1TE"
   },
   "outputs": [],
   "source": [
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "\n",
    "def download_pretrained_vgg(data_dir):\n",
    "    vgg_filename = 'vgg.zip'\n",
    "    vgg_path = os.path.join(data_dir, 'vgg')\n",
    "    vgg_files = [\n",
    "        os.path.join(vgg_path, 'variables/variables.data-00000-of-00001'),\n",
    "        os.path.join(vgg_path, 'variables/variables.index'),\n",
    "        os.path.join(vgg_path, 'saved_model.pb')]\n",
    "\n",
    "    missing_vgg_files = [vgg_file for vgg_file in vgg_files if not os.path.exists(vgg_file)]\n",
    "    if missing_vgg_files:\n",
    "        if os.path.exists(vgg_path):\n",
    "            shutil.rmtree(vgg_path)\n",
    "        os.makedirs(vgg_path)\n",
    "\n",
    "        print('Downloading pre-trained vgg model...')\n",
    "        with DLProgress(unit='B', unit_scale=True, miniters=1) as pbar:\n",
    "            urlretrieve(\n",
    "                'https://s3-us-west-1.amazonaws.com/udacity-selfdrivingcar/vgg.zip',\n",
    "                os.path.join(vgg_path, vgg_filename),\n",
    "                pbar.hook)\n",
    "\n",
    "        print('Extracting model...')\n",
    "        zip_ref = zipfile.ZipFile(os.path.join(vgg_path, vgg_filename), 'r')\n",
    "        zip_ref.extractall(data_dir)\n",
    "        zip_ref.close()\n",
    "\n",
    "        os.remove(os.path.join(vgg_path, vgg_filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузка VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l9nYCa3dddpM"
   },
   "outputs": [],
   "source": [
    "download_pretrained_vgg(vgg16_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PDtB9bERMwHP"
   },
   "source": [
    "<a name=\"4\"></a>\n",
    "## 4. Работа с данными"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zP8rSa6Addpr"
   },
   "outputs": [],
   "source": [
    "# функция для считывания пары изображений датасета-исходное изображение и размеченное\n",
    "def get_dataset_image(dataset_folder, image_shape):\n",
    "    data_folder = os.path.join(dataset_folder, \"training\")\n",
    "    \n",
    "    image_paths = glob(os.path.join(data_folder, \"image_2\", \"*.png\"))\n",
    "    label_paths = {\n",
    "        re.sub(r\"_(lane|road)_\", \"_\", os.path.basename(path)): path\n",
    "        for path in glob(os.path.join(data_folder, \"gt_image_2\", \"*_road_*.png\"))}\n",
    "    background_color = np.array([255, 0, 0])\n",
    "    \n",
    "    image_file = np.random.choice(image_paths)\n",
    "    \n",
    "    gt_image_file = label_paths[os.path.basename(image_file)]\n",
    "                \n",
    "    image = scipy.misc.imresize(scipy.misc.imread(image_file), image_shape)\n",
    "    gt_image = scipy.misc.imresize(scipy.misc.imread(gt_image_file), image_shape)\n",
    "    \n",
    "    return np.array(image), np.array(gt_image)\n",
    "\n",
    "\n",
    "# вывод рандомного обучающего примера из датасета\n",
    "def show_sample(path, image_shape):\n",
    "    x_train_img, y_train_img = get_dataset_image(path, image_shape)\n",
    "    fig=plt.figure(figsize=(25, 25))\n",
    "    fig.add_subplot(1,2,1)\n",
    "    plt.imshow(x_train_img)\n",
    "    fig.add_subplot(1,2,2)\n",
    "    plt.imshow(y_train_img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вывод пары (исходное изображение, сегментированное изображение) из обучающего датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_30kjLKEOHJQ"
   },
   "outputs": [],
   "source": [
    "show_sample(dataset_path, (375, 1242))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для обучения сети нужно произвести преобразование каждого пикселя размеченного изображения из обучающего набора в one-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Исходное представление\n",
    "<img src=\"images/fcn8-kittiroad/seg.png\" width=\"900px\">\n",
    "<br>\n",
    "One-hot encoding представление\n",
    "<img src=\"images/fcn8-kittiroad/one_hot_seg.png\" width=\"900px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# генератор изображений для обучения сети\n",
    "def gen_batch_function(dataset_folder, image_shape):\n",
    "    data_folder = os.path.join(dataset_folder, \"training\")\n",
    "    # получение очередного батча\n",
    "    def get_batches_fn(batch_size):\n",
    "        image_paths = glob(os.path.join(data_folder, 'image_2', '*.png'))\n",
    "        label_paths = {\n",
    "            re.sub(r'_(lane|road)_', '_', os.path.basename(path)): path\n",
    "            for path in glob(os.path.join(data_folder, 'gt_image_2', '*_road_*.png'))}\n",
    "        background_color = np.array([255, 0, 0])\n",
    "\n",
    "        random.shuffle(image_paths)\n",
    "        # расчет количества батчей исходя из размера батча\n",
    "        for batch_i in range(0, len(image_paths), batch_size):\n",
    "            images = []\n",
    "            gt_images = []\n",
    "            # загрузка изображений в батч\n",
    "            for image_file in image_paths[batch_i:batch_i+batch_size]:\n",
    "                gt_image_file = label_paths[os.path.basename(image_file)]\n",
    "\n",
    "                image = scipy.misc.imresize(scipy.misc.imread(image_file), image_shape)\n",
    "                gt_image = scipy.misc.imresize(scipy.misc.imread(gt_image_file), image_shape)\n",
    "\n",
    "                # Преобразуем из RGB размерности в one-hot encoding формат\n",
    "                # т.е. каждый пиксель будет категориальным\n",
    "                # [[[0,1]-категориальный пиксель в строке,[1,0],[0,1]...]-строка, [[0,1],[1,0],[0,1]...], ...]\n",
    "                gt_bg = np.all(gt_image == background_color, axis=2)\n",
    "                gt_bg = gt_bg.reshape(*gt_bg.shape, 1)\n",
    "                gt_image = np.concatenate((gt_bg, np.invert(gt_bg)), axis=2)\n",
    "\n",
    "                images.append(image)\n",
    "                gt_images.append(gt_image)\n",
    "\n",
    "            yield np.array(images), np.array(gt_images)\n",
    "    return get_batches_fn\n",
    "\n",
    "\n",
    "# генератор изображений для теста сети\n",
    "def gen_test_output(sess, logits, keep_prob, image_pl, data_folder, image_shape):\n",
    "    for image_file in glob(os.path.join(data_folder, 'image_2', '*.png')):\n",
    "        image = scipy.misc.imresize(scipy.misc.imread(image_file), image_shape)\n",
    "        #image = skimage.img_as_ubyte(transform.resize(scipy.misc.imread(image_file), image_shape))\n",
    "\n",
    "        # прогоняем изображение через модель\n",
    "        im_softmax = sess.run(\n",
    "            [tf.nn.softmax(logits)],\n",
    "            {keep_prob: 1.0, image_pl: [image]})\n",
    "        im_softmax = im_softmax[0][:, 1].reshape(image_shape[0], image_shape[1])\n",
    "        # применяем порог к вероятностям softmax\n",
    "        segmentation = (im_softmax > 0.5).reshape(image_shape[0], image_shape[1], 1)\n",
    "        # формируем маску сегментации\n",
    "        mask = np.dot(segmentation, np.array([[0, 255, 0, 127]]))\n",
    "        mask = scipy.misc.toimage(mask, mode=\"RGBA\")\n",
    "        # накладываем маску сегментации на изображение\n",
    "        street_im = scipy.misc.toimage(image)\n",
    "        street_im.paste(mask, box=None, mask=mask)\n",
    "\n",
    "        yield os.path.basename(image_file), np.array(street_im)\n",
    "\n",
    "        \n",
    "# сегментирование тестовых изображений с помощью обученной сети и сохранение\n",
    "def save_inference_samples(runs_dir, data_dir, sess, image_shape, logits, keep_prob, input_image):\n",
    "    output_dir = os.path.join(runs_dir, str(time.time()))\n",
    "    if os.path.exists(output_dir):\n",
    "        shutil.rmtree(output_dir)\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "    print('Saving test images to: {}, please wait...'.format(output_dir))\n",
    "    image_outputs = gen_test_output(\n",
    "        sess, logits, keep_prob, input_image, os.path.join(data_dir, 'testing'), image_shape)\n",
    "    for name, image in image_outputs:\n",
    "         scipy.misc.imsave(os.path.join(output_dir, name), image)\n",
    "\n",
    "    print('All images are saved!')\n",
    "\n",
    "# сегментация рандомного образца тестовой выборки\n",
    "def test_random_sample(data_dir, sess, image_shape, logits, keep_prob, image_pl):\n",
    "    images = glob(os.path.join(os.path.join(data_dir, 'testing'), 'image_2', '*.png'))\n",
    "    image_file = np.random.choice(images)\n",
    "    image = scipy.misc.imresize(scipy.misc.imread(image_file), image_shape)\n",
    "    \n",
    "    # прогоняем изображение через модель\n",
    "    im_softmax = sess.run(\n",
    "        [tf.nn.softmax(logits)],\n",
    "        {keep_prob: 1.0, image_pl: [image]})\n",
    "    im_softmax = im_softmax[0][:, 1].reshape(image_shape[0], image_shape[1])\n",
    "    \n",
    "    # применяем порог к вероятностям softmax\n",
    "    segmentation = (im_softmax > 0.5).reshape(image_shape[0], image_shape[1], 1)\n",
    "    \n",
    "    # формируем маску сегментации\n",
    "    mask = np.dot(segmentation, np.array([[0, 255, 0, 127]]))\n",
    "    mask = scipy.misc.toimage(mask, mode=\"RGBA\")\n",
    "    \n",
    "    # накладываем маску сегментации на изображение\n",
    "    street_im = scipy.misc.toimage(image)\n",
    "    street_im.paste(mask, box=None, mask=mask)\n",
    "    \n",
    "    return os.path.basename(image_file), np.array(street_im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"5\"></a>\n",
    "## 5. Полностью сверточная нейронная сеть FCN8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Схема полностью сверточной нейронной сети FCN8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/fcn8-kittiroad/fcn.png\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/fcn8-kittiroad/fcn_8.png\" width=\"900px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FCN8 имеет архитектуру кодер-декодер (encoder-decoder)<br>\n",
    "Кодер - предварительно обученная нейронная сеть классификации VGG16 без полносязных слоев<br>\n",
    "Декодер - транспонированная свертка (transpose convolution) и соединения (skip-connections) с соответствующими картами кодера"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"5a\"></a>\n",
    "### a. Кодер "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Структура сети VGG16 (для кодера FC слои исключаются из сети)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/fcn8-kittiroad/vgg16.png\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"5b\"></a>\n",
    "### b. Декодер"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Организация работы декодера"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/fcn8-kittiroad/fcn8.png\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получим входной слой, регуляризацию, выходные слои 3,4,7 сети VGG16 (в загруженной ранее сети vgg16 уже убраны FC слои и вместо них добавлены слои свертки 6-7 в соответствии со схемой сети FCN8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vgg(sess, vgg_path):\n",
    "    tf.saved_model.loader.load(sess, ['vgg16'], vgg_path)\n",
    "    image_input = tf.get_default_graph().get_tensor_by_name('image_input:0')\n",
    "    keep_prob = tf.get_default_graph().get_tensor_by_name('keep_prob:0')\n",
    "    vgg_layer3_out = tf.get_default_graph().get_tensor_by_name('layer3_out:0')\n",
    "    vgg_layer4_out = tf.get_default_graph().get_tensor_by_name('layer4_out:0')\n",
    "    vgg_layer7_out = tf.get_default_graph().get_tensor_by_name('layer7_out:0')\n",
    "\n",
    "    return image_input, keep_prob, vgg_layer3_out, vgg_layer4_out, vgg_layer7_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Формируем декодер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layers(vgg_layer3_out, vgg_layer4_out, vgg_layer7_out, num_classes):\n",
    "    # свертка 1х1 по всем картам признаков слоев 3,4,7 сети VGG16\n",
    "    # применяется для лучшего обучения сети, а также повышает точность сегментации\n",
    "    vgg_layer7_logits = tf.layers.conv2d(\n",
    "        vgg_layer7_out, num_classes, kernel_size=1,\n",
    "        kernel_initializer= tf.random_normal_initializer(stddev=0.01),\n",
    "        kernel_regularizer= tf.contrib.layers.l2_regularizer(1e-4), name='vgg_layer7_logits')\n",
    "    vgg_layer4_logits = tf.layers.conv2d(\n",
    "        vgg_layer4_out, num_classes, kernel_size=1,\n",
    "        kernel_initializer= tf.random_normal_initializer(stddev=0.01),\n",
    "        kernel_regularizer= tf.contrib.layers.l2_regularizer(1e-4), name='vgg_layer4_logits')\n",
    "    vgg_layer3_logits = tf.layers.conv2d(\n",
    "        vgg_layer3_out, num_classes, kernel_size=1,\n",
    "        kernel_initializer= tf.random_normal_initializer(stddev=0.01),\n",
    "        kernel_regularizer= tf.contrib.layers.l2_regularizer(1e-4), name='vgg_layer3_logits')\n",
    "\n",
    "    # transpose convolutions и skip-connections\n",
    "    fcn_decoder_layer1 = tf.layers.conv2d_transpose(\n",
    "        vgg_layer7_logits, num_classes, kernel_size=4, strides=(2, 2),\n",
    "        padding='same',\n",
    "        kernel_initializer= tf.random_normal_initializer(stddev=0.01),\n",
    "        kernel_regularizer= tf.contrib.layers.l2_regularizer(1e-4), name='fcn_decoder_layer1')\n",
    "\n",
    "    fcn_decoder_layer2 = tf.add(\n",
    "        fcn_decoder_layer1, vgg_layer4_logits, name='fcn_decoder_layer2')\n",
    "\n",
    "    fcn_decoder_layer3 = tf.layers.conv2d_transpose(\n",
    "        fcn_decoder_layer2, num_classes, kernel_size=4, strides=(2, 2),\n",
    "        padding='same',\n",
    "        kernel_initializer= tf.random_normal_initializer(stddev=0.01),\n",
    "        kernel_regularizer= tf.contrib.layers.l2_regularizer(1e-4), name='fcn_decoder_layer3')\n",
    "\n",
    "    fcn_decoder_layer4 = tf.add(\n",
    "        fcn_decoder_layer3, vgg_layer3_logits, name='fcn_decoder_layer4')\n",
    "    fcn_decoder_output = tf.layers.conv2d_transpose(\n",
    "        fcn_decoder_layer4, num_classes, kernel_size=16, strides=(8, 8),\n",
    "        padding='same',\n",
    "        kernel_initializer= tf.random_normal_initializer(stddev=0.01),\n",
    "        kernel_regularizer= tf.contrib.layers.l2_regularizer(1e-4), name='fcn_decoder_layer4')\n",
    "\n",
    "    return fcn_decoder_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"5c\"></a>\n",
    "### с. Обучение и тестирование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция оптимизации - в качестве функции потерь взята попиксельная кросс-энтропийная потеря, оптимизатор Адам"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/fcn8-kittiroad/pix_ce_loss.png\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(nn_last_layer, correct_label, learning_rate, num_classes):\n",
    "    # преобразование в 2D one-hot encoding представление\n",
    "    logits = tf.reshape(nn_last_layer, (-1, num_classes))\n",
    "    correct_label = tf.reshape(correct_label, (-1, num_classes))\n",
    "   \n",
    "    cross_entropy_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=correct_label))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate= learning_rate)\n",
    "    train_op = optimizer.minimize(cross_entropy_loss)\n",
    "\n",
    "    return logits, train_op, cross_entropy_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция обучения нейронной сети FCN8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xIBuh-bponvQ"
   },
   "outputs": [],
   "source": [
    "def train_nn(sess, epochs, batch_size, get_batches_fn, train_op, cross_entropy_loss, input_image,\n",
    "             correct_label, keep_prob, learning_rate):\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    print(\"Training...\")\n",
    "    print()\n",
    "    for i in range(epochs):\n",
    "        print(\"Epoch {} ...\".format(i+1))\n",
    "        training_loss = 0\n",
    "        training_samples = 0\n",
    "        starttime = time.clock()\n",
    "        for image, label in get_batches_fn(batch_size):\n",
    "            _, loss = sess.run([train_op, cross_entropy_loss],\n",
    "                               feed_dict={input_image: image, correct_label: label,\n",
    "                                          keep_prob: dropout, learning_rate: lr})\n",
    "            training_samples += 1\n",
    "            training_loss += loss\n",
    "\n",
    "        training_loss /= training_samples\n",
    "        endtime = time.clock()\n",
    "        training_time = endtime-starttime\n",
    "\n",
    "        print(\"Loss: = {:.3f}\\n\".format(training_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запускаем пайплайн создания сети FCN8 и ее обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zT8Sr5e-VMp6"
   },
   "outputs": [],
   "source": [
    "# получение сессии\n",
    "sess = tf.Session()\n",
    "# генератор изображений для обучения\n",
    "get_batches_fn = gen_batch_function(dataset_path, image_shape)\n",
    "\n",
    "# TF placeholders\n",
    "correct_label = tf.placeholder(tf.int32, [None, None, None, num_classes], name='correct_label')\n",
    "learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "\n",
    "# Получение входного слоя, регуляризации, выходных слоев 3,4,7 сети VGG16\n",
    "input_image, keep_prob, vgg_layer3_out, vgg_layer4_out, vgg_layer7_out = load_vgg(sess, vgg16_path + \"/vgg\")\n",
    "\n",
    "# Построение FCN8 используя слои VGG16\n",
    "nn_last_layer = layers(vgg_layer3_out, vgg_layer4_out, vgg_layer7_out, num_classes)\n",
    "\n",
    "logits, train_op, cross_entropy_loss = optimize(nn_last_layer, correct_label, learning_rate, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучение сети\n",
    "train_nn(sess, epochs, batch_size, get_batches_fn, train_op, cross_entropy_loss, input_image, \n",
    "         correct_label, keep_prob, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохранение сети в файл\n",
    "#saver = tf.train.Saver()\n",
    "#save_path = saver.save(sess, model_path)\n",
    "#print(\"Model is saved to file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вывод рандомного тестового образца сегментированного обученной нейронной сетью FCN8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nD0wxAt1tbhJ"
   },
   "outputs": [],
   "source": [
    "test_name, test_img = test_random_sample(dataset_path, sess, image_shape, logits, keep_prob, input_image)\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(test_img)\n",
    "\n",
    "# сегментация всех тестовых изображений с помощью обученной fcn8 и их сохранение\n",
    "# save_inference_samples(out_path, dataset_path, sess, image_shape, logits, keep_prob, input_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T2ROvt-nxJVs"
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"6\"></a>\n",
    "## 6. Источники"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Датасет KittiRoad: https://s3.eu-central-1.amazonaws.com/avg-kitti/data_road.zip<br>\n",
    "Предобученная сверточная нейронная сеть классификации VGG16: https://s3-us-west-1.amazonaws.com/udacity-selfdrivingcar/vgg.zip<br>\n",
    "Road Segmentation: https://junshengfu.github.io/semantic_segmentation/<br>\n",
    "An Overview of semantic image segmentation: https://www.jeremyjordan.me/semantic-segmentation/<br>\n",
    "Fully Convolutional Networks for Semantic Segmentation: https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf <br>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "fcn8_kittyroad_tensorflow (1).ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
